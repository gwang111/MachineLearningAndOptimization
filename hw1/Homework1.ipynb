{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOn7a5hHO+LoZgTRskHirQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwang111/MachineLearningAndOptimization/blob/main/hw1/Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noC_XhIpwfA7",
        "outputId": "cb1a13cd-08d5-4e22-9836-1d553ae65182"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oocPstBf6Du2"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "\n",
        "# For Question 2\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For Question 3\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtHo95in71hF"
      },
      "source": [
        "1.\n",
        "\n",
        "The code block below:\n",
        "\n",
        "1.   Reads in the spam.csv from the most immediate directory. Making sure the coding is for English\n",
        "2.   Next we select all the rows and the first two columns and assign that as the dataframe\n",
        "3.  Next replace all instances of ham in the df as 0 and then replace all instances of spam in the df as 1\n",
        "4.  Next rename the columns v1 to label and v2 to message\n",
        "5.  Finally drop duplicate rows and then reset the indexes accounting for the dropped rows\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8L-3Hgw6ezX"
      },
      "source": [
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df.iloc[:, :2]\n",
        "df = df.replace('ham', 0).replace('spam', 1)\n",
        "df = df.rename(columns = {'v1': 'label', 'v2': 'message'})\n",
        "df = df.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPA2mABw9qx_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "411f8910-9da2-47f6-aa3f-64bc23d6227f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5164</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5165</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5169 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                            message\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro...\n",
              "...     ...                                                ...\n",
              "5164      1  This is the 2nd time we have tried 2 contact u...\n",
              "5165      0              Will Ì_ b going to esplanade fr home?\n",
              "5166      0  Pity, * was in mood for that. So...any other s...\n",
              "5167      0  The guy did some bitching but I acted like i'd...\n",
              "5168      0                         Rofl. Its true to its name\n",
              "\n",
              "[5169 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsuOsrBEmtST"
      },
      "source": [
        "2.\n",
        "\n",
        "**Tokenizing data:** Given theses string, the act of tokenizing is to split the inputs up into keywords, elements, etc that are referred to as tokens.\n",
        "\n",
        "**CountVectorizer:** For CountVectorizer, using the default token_pattern input, 2 or more alphanumeric characters in a row are considered a token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWA7pUNxwzEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f38e93-c193-4d53-db38-d5c019515466"
      },
      "source": [
        "messages = df['message']\n",
        "y = df['label']\n",
        "x = CountVectorizer().fit_transform(messages)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5169x8672 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 68018 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnYt2jZVWfDw"
      },
      "source": [
        "**Unique Tokens Identified**: 4767\n",
        "This was found by summing up all the columns and then counting how many occurances of 1 occurred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGJUOrwVUaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487b2ea0-a9eb-4408-cbee-9435441dcf59"
      },
      "source": [
        "token_occ = x.toarray().sum(axis=0)\n",
        "\n",
        "print(token_occ.shape)\n",
        "\n",
        "unique = 0\n",
        "for i in token_occ:\n",
        "    if i == 1:\n",
        "        unique = unique + 1\n",
        "\n",
        "# unique words\n",
        "print(unique)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8672,)\n",
            "4767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSIFASw3hrYE"
      },
      "source": [
        "Analyzing the training data, the training set does not contain at least one instance of each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvA8nir3hFmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fff9136-86bd-42c2-d647-bfe4fd7b0122"
      },
      "source": [
        "train_occ = x_train.toarray().sum(axis=0)\n",
        "\n",
        "print(train_occ.shape)\n",
        "\n",
        "atleast_one = True\n",
        "for i in train_occ:\n",
        "    if i == 0: \n",
        "        atleast_one = False        \n",
        "        break\n",
        "\n",
        "print(atleast_one)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8672,)\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiCNyAL1kSco"
      },
      "source": [
        "3.\n",
        "\n",
        "**Accuracy**: All ML models in sklearn have a score method. This score method takes the model that we've trained using the x and y training data and then when score is called with x and y test passed in, it calculates how accurate the model was in predicting y from x. Accuracy is calculated as:\n",
        "`number of correct predictions / total predictions` (i.e. the percentage of samples that the model was able to successfully predict). This is an important metric because this gives us an immediate idea of how good our machine learning model at doing its job (which is to find spam). For example, if the model correctly predicts 90% of the test set, we could potentially say that our model will give us the correct spam classification 90% of the time (how well the model did at marking spam as spam and ham as). But to get a better in-depth understanding of how well our model did, we must use precision and recall to further assess this.\n",
        "\n",
        "**Precision**: `tp / (tp + fp)` where `tp` is the number of true positives and `fp` is the number of false positives. In other words, precision assesses the percentage of text messages classified as spam that were actually spam. Precision is very important because it is not enough to correctly classify all spam samples as spam. It is equally important that ham samples are not incorrectly being classified as spam (negatively impacts customers when a filter is filtering out important and relevant text messages).\n",
        "\n",
        "**Recall**: `tp / (tp + fn)` where `tp` is the number of true positives (spam as spam) and `fn` is the number of false negatives (spam as ham). In other words, recall assesses the percentage of actual spam text messages that were correctly classified as spam. Recall is very important because we want to find all spam text messages (negatively impacts customers if spam text messages get through the filter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhHxkDMCkRyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f29b80c-e1d5-4a32-c6f2-86ad863eab9b"
      },
      "source": [
        "mnb_clf = MultinomialNB()\n",
        "mnb_clf.fit(x_train, y_train)\n",
        "y_hat = mnb_clf.predict(x_test)\n",
        "[mnb_clf.score(x_test, y_test), precision_score(y_test, y_hat), recall_score(y_test, y_hat)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9777562862669246, 0.9154929577464789, 0.9219858156028369]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRcYYDFEWKNK"
      },
      "source": [
        "4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI9Abf2IBv1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d845b656-1510-4119-93eb-0e2be19bff5d"
      },
      "source": [
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(x_train, y_train)\n",
        "y_hat = linear_svc.predict(x_test)\n",
        "[linear_svc.score(x_test, y_test), precision_score(y_test, y_hat), recall_score(y_test, y_hat)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9816247582205029, 0.9919354838709677, 0.8723404255319149]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5MI_3EpWLuF"
      },
      "source": [
        "5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6yc36olBzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56768b95-0826-45c2-f219-28b5a2610662"
      },
      "source": [
        "logistic_reg = LogisticRegression()\n",
        "logistic_reg.fit(x_train, y_train)\n",
        "y_hat = logistic_reg.predict(x_test)\n",
        "[logistic_reg.score(x_test, y_test), precision_score(y_test, y_hat), recall_score(y_test, y_hat)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9787234042553191, 0.983739837398374, 0.8581560283687943]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bZzGCfVWBGS"
      },
      "source": [
        "6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2ujNEXKHfh"
      },
      "source": [
        "|| Accuracy | Precision | Recall |\n",
        "| --- | --- | --- | --- |\n",
        "| MultinomialNB | 0.9777562862669246 | 0.9154929577464789 | 0.9219858156028369 |\n",
        "| LinearSVC | 0.9816247582205029 | 0.9919354838709677 | 0.8723404255319149 |\n",
        "| LogisticRegression | 0.9787234042553191 | 0.983739837398374 | 0.8581560283687943 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMzZ3WQu18o9"
      },
      "source": [
        "- I think what suprised me was the difference in Recall, namely 92% in MultinomialNB to 87% and 86% from LinearSVC and LogisticRegression respectively. (A 5% and 6% difference). With LinearSVC and LogisticRegression having lower recall but precision extremely high this ment that these two models were doing a great job at correctly assessing spam as spam but were classifying some spam samples as ham. This shows that LinearSVC and LogisticRegression seemed less sensative to the data. This would seem expected with LinearSVC because of an implemented thickness to counteract noise and overfitting but seems unexpected with LogisticRegression as LogisticRegression can be prone to overfitting.\n",
        "- I think I would use MultinomialNB because for one, it performs well with NLP binary classification problems but also gives us more consistant percentages than LinearSVC and LogisticRegression with all 3 percentages in the 90s. Furthermore, while all 3 percentages are above 90%, it does a good job in this example of not being overly exact, this may be good when apply this model to various other text datasets as were are not overfitted as much as LinearSVC and LogisticRegression are to this spam.csv set. I'd rather my model be both able to correctly classify spam well and also be able to find all spam texts which MultinomialNB seems balance better than LinearSVC and LogisticRegression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPJ_YXxnPqXw"
      },
      "source": [
        "7.\n",
        "- One caviate to consider is how informal the ham texts are (seemingly originating and collected from casual text conversations between familiar people). We also note how extremely formal and robotic the spam texts are consistantly like. With spam texts becoming more sophisticated, people formating them with more humanlike diction, using this dataset may not give us enough coverage on what is spam and what is ham.\n",
        "- Another caviate is that some of these messages (specifcally spam messages) are extremely outdated (i.e messages referencing the year 2003, Nokia 3510i, 3G connection, etc.). These messages may not be relevant or apply to the year 2021 as diction and spam techniques have greatly changed since then.\n",
        "- We must be wary that our model trained on this dataset may not be transferable when applied on text data that we have collected from 2021."
      ]
    }
  ]
}